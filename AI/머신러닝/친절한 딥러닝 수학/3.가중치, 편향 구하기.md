신경망 = 인간의 두뇌를 흉내낸 머신러닝의 한 기술
유닛 하나하나가 뉴런을 흉내낸다
유닛간의 연결에 가중치가 있고 가중치와 편향을 최적화하는것이 모델을 학습시키는 것이다

은닉층이 여러개인 신경망을 심층신경망이라하고 심층신경망의 가중치, 편향을 최적화하는것이 딥러닝이다
가중치, 편향을 구하기위해 딥러닝은 경사하강법을 이용한다 (? 이용하는게 있다?)

---

## 목적함수, 최적화문제
신경망의 오차를 최소로하는 가중치와 편향을 구한다
오차를 나타낸 오차함수 -> 목적함수
목적함수를 미분해서 목적함수를 최소화하는 가중치와 편향을 구한다, 

> 이것을 최적화문제를 해결한다함 

목적함수는 오류함수, 비용함수, 손실함수 등 목적에따라 다른이름을 붙이는것같다.(?)

가중치와 편향이 엄청 많기때문에 목적함수는 다변수함수가 되고 따라서 편미분을 한다


## 경사하강법

함수의 경사를 따라 하강하며 함수를 최소로 만드는 파라미터를 찾는 방법

최소화 하려는 함수를 정의한후
임의로 선택한 파라미터에서 시작하여 함수의 기울기를 계산한다
기울기의 반대부호로 파라미터를 업데이트하고 이과정을 반복해서 함수의 최솟값을 찾는다
한번의 업데이트에서 학습률을 적용하는데 학습률이 너무크면 최적점을 지나쳐버리고 학습률이 너무 작으면 최적의 파라미터를 찾는데 오래걸리기 때문에 적절한 학습률을 사용해야한다


목적함수를 입력층에서부터 미분하면 합성함수의 미분이 되기 때문에 식이 엄청 복잡해진다. 출력층에서부터 미분하면 식이 훨씬 간단해지기때문에 뒤에서부터 계산하는것을 오차역전파법이라 한다

신경망의 활성화함수로 계단함수를 이용하면 미분했을때 모두 0이 되기때문에 경사하강법에서 파라미터를 갱신할수 없게된다 따라서 미분했을때 0이 되지않는 시그모이드 함수를 이용하고 시그모이드 함수도 x가 0에서 멀어질수록 미분했을때 기울기가 0이되는 기울기 소실문제가 있기 때문에 ReLu함수를 활성화함수로 쓴다

Rectified Linear Unit (정류된 선형 함수)



